{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4","mount_file_id":"1qmgQgLKq5ODFJ_IT4BjYjTDZq4Bc5CeA","authorship_tag":"ABX9TyMjWHJo8aUy4EnDWkROn5JR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"66e15b43f11f43c8a34e87cd1dd53e16":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9a4fc2a26d274e7e99976a36e80d7755","IPY_MODEL_95f5a828833e4e9890e665451550705a","IPY_MODEL_13bafed626e84e58bf0cead01dd48789"],"layout":"IPY_MODEL_c4e5fed8ecfd42fd97bcd09cf730ff29"}},"9a4fc2a26d274e7e99976a36e80d7755":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_215ccd2b0b48468dac12712ad5995b64","placeholder":"‚Äã","style":"IPY_MODEL_7549c916e1dd460ab9846cebe977921e","value":"Map:‚Äá100%"}},"95f5a828833e4e9890e665451550705a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ff89664c6e8c4ef89eb047eb01d72833","max":180,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cd9a4b4a3c074d98b04b6c563ef4af09","value":180}},"13bafed626e84e58bf0cead01dd48789":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d5687f0a4fdc470bb15c2a57684190e2","placeholder":"‚Äã","style":"IPY_MODEL_a5328e4b8f9d42f7b133de04064e44f4","value":"‚Äá180/180‚Äá[00:00&lt;00:00,‚Äá1368.83‚Äáexamples/s]"}},"c4e5fed8ecfd42fd97bcd09cf730ff29":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"215ccd2b0b48468dac12712ad5995b64":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7549c916e1dd460ab9846cebe977921e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ff89664c6e8c4ef89eb047eb01d72833":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cd9a4b4a3c074d98b04b6c563ef4af09":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d5687f0a4fdc470bb15c2a57684190e2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a5328e4b8f9d42f7b133de04064e44f4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e7ca88f02179402f9ffd1f29a270e722":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ea88385680c64d7a821b100cf6d676e8","IPY_MODEL_79544125cb454f04955faa2b5d5f995e","IPY_MODEL_b1f20ccb710f49c6b01d1ec9bb10b4f7"],"layout":"IPY_MODEL_95eafa55b6c1446bb544186cb760a36d"}},"ea88385680c64d7a821b100cf6d676e8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d120ac3dbf78496691da2155d428d1d8","placeholder":"‚Äã","style":"IPY_MODEL_c3b9c903fc344603abf0346b6edb9335","value":"Map:‚Äá100%"}},"79544125cb454f04955faa2b5d5f995e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0cd47509ba1b47e2ad333dd635445909","max":1440,"min":0,"orientation":"horizontal","style":"IPY_MODEL_16ce3bc5edf24f8f962ba3618bf75f70","value":1440}},"b1f20ccb710f49c6b01d1ec9bb10b4f7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9c5019afe3e546f48334072e57dce64a","placeholder":"‚Äã","style":"IPY_MODEL_af9882eeb0554e17b5f5b0685b492f58","value":"‚Äá1440/1440‚Äá[00:00&lt;00:00,‚Äá1477.47‚Äáexamples/s]"}},"95eafa55b6c1446bb544186cb760a36d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d120ac3dbf78496691da2155d428d1d8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c3b9c903fc344603abf0346b6edb9335":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0cd47509ba1b47e2ad333dd635445909":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"16ce3bc5edf24f8f962ba3618bf75f70":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9c5019afe3e546f48334072e57dce64a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"af9882eeb0554e17b5f5b0685b492f58":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e69f8f50f33e42d089018cf2a80430b3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7d1ee7b85dcf4921aaccee7697666f1e","IPY_MODEL_79701a682f3145fc95c748bdaa0f1c34","IPY_MODEL_2f5b8fe006c94dca8c9af24131453a2f"],"layout":"IPY_MODEL_39c85797b6f444c396068c62e99dc038"}},"7d1ee7b85dcf4921aaccee7697666f1e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_355b9e871d1343edb88861a5b0d49994","placeholder":"‚Äã","style":"IPY_MODEL_79438f09899445749d53311bb5bb0589","value":"Map:‚Äá100%"}},"79701a682f3145fc95c748bdaa0f1c34":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cf2e73f829164f3fb686d3cd71c5e078","max":180,"min":0,"orientation":"horizontal","style":"IPY_MODEL_23f6a8f6ca14402fba7a2ec9860acd43","value":180}},"2f5b8fe006c94dca8c9af24131453a2f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2824a7db603f4fbda6f17d2eb5273a16","placeholder":"‚Äã","style":"IPY_MODEL_9db9e040ef054a978af2e5dfbb3cc804","value":"‚Äá180/180‚Äá[00:00&lt;00:00,‚Äá1378.40‚Äáexamples/s]"}},"39c85797b6f444c396068c62e99dc038":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"355b9e871d1343edb88861a5b0d49994":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"79438f09899445749d53311bb5bb0589":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cf2e73f829164f3fb686d3cd71c5e078":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"23f6a8f6ca14402fba7a2ec9860acd43":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2824a7db603f4fbda6f17d2eb5273a16":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9db9e040ef054a978af2e5dfbb3cc804":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d66069a13e1c4403ae03049e4fe9a279":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3a1fe2d8058d401c9f71fc3ee8324771","IPY_MODEL_b952abe1968140dea536d00b003f4183","IPY_MODEL_fde1ca3e5136410cb151887ca5c06098"],"layout":"IPY_MODEL_13c4e82a47094a02a8005731f8dd9835"}},"3a1fe2d8058d401c9f71fc3ee8324771":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a34c2925eddb4062828507e92e3565fc","placeholder":"‚Äã","style":"IPY_MODEL_f6e84d95490e4cbb92e2a0e733331b06","value":"Map:‚Äá100%"}},"b952abe1968140dea536d00b003f4183":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ef3879e322954a01a885714811e1169e","max":180,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f8a9ea8386da4148bb26e90de8192bad","value":180}},"fde1ca3e5136410cb151887ca5c06098":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e10db449a6284d6d9a3deeda1214883c","placeholder":"‚Äã","style":"IPY_MODEL_363da69fa6b241d1840de57707debc20","value":"‚Äá180/180‚Äá[00:00&lt;00:00,‚Äá1369.51‚Äáexamples/s]"}},"13c4e82a47094a02a8005731f8dd9835":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a34c2925eddb4062828507e92e3565fc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f6e84d95490e4cbb92e2a0e733331b06":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ef3879e322954a01a885714811e1169e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f8a9ea8386da4148bb26e90de8192bad":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e10db449a6284d6d9a3deeda1214883c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"363da69fa6b241d1840de57707debc20":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# **Finetuning del modelo T5**"],"metadata":{"id":"nNrKLRZQpV_U"}},{"cell_type":"markdown","source":["En este notebook llevamos a cabo el finetuning del modelo T5. Se trata de un proceso complejo, que hubo que corregir varias veces, debido a la incompatibilidad de librer√≠as, en especial con accelerate, y problemas con la tokenizaci√≥n."],"metadata":{"id":"HDxh_njjqFuD"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"U_ASu70fJ3Ro"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["Comenzamos con la instalaci√≥n de las librer√≠as, que ya digo que dieron problemas."],"metadata":{"id":"RsvyTXHErRC8"}},{"cell_type":"code","source":["\"\"\"\n","FINE-TUNING T5 PARA AN√ÅLISIS ELECTORAL ESPA√ëOL\n","Google Colab Pro - VERSI√ìN ESTABLE Y CORREGIDA\n","Con soluci√≥n integrada para error \"piece id is out of range\"\n","\"\"\"\n","\n","# ============================================================================\n","# CELDA 1: INSTALACI√ìN DE DEPENDENCIAS (VERSI√ìN ESTABLE)\n","# ============================================================================\n","print(\"üöÄ INICIANDO FINE-TUNING T5 - VERSI√ìN ESTABLE\")\n","print(\"=\" * 70)\n","\n","# Instalaci√≥n de dependencias COMPATIBLES\n","!pip install -q transformers==4.36.0\n","!pip install -q accelerate==0.21.0  # Specify compatible accelerate version\n","!pip install -q datasets==2.15.0\n","!pip install -q evaluate==0.4.0\n","!pip install -q rouge-score==0.1.2\n","!pip install -q nltk==3.8.1\n","!pip install -q sentencepiece==0.1.99\n","!pip install -q protobuf==3.20.3\n","!pip install -q peft==0.5.0\n","\n","import os\n","import sys\n","import json\n","import torch\n","import numpy as np\n","import pandas as pd\n","from datetime import datetime\n","import gc\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","print(\"‚úÖ Dependencias instaladas\")\n","print(f\"PyTorch: {torch.__version__}\")\n","print(f\"CUDA disponible: {torch.cuda.is_available()}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vT2ka-1pKUJ7","executionInfo":{"status":"ok","timestamp":1770134272883,"user_tz":-60,"elapsed":41076,"user":{"displayName":"Guillermo Barrio","userId":"06925154007558381895"}},"outputId":"eb477c30-e284-4a63-cf54-803a6af71005"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["üöÄ INICIANDO FINE-TUNING T5 - VERSI√ìN ESTABLE\n","======================================================================\n","‚úÖ Dependencias instaladas\n","PyTorch: 2.9.0+cu126\n","CUDA disponible: True\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"DVcjMaA0KUG5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"dzkQUDqSKUEB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["DeepSeek incluy√≥ esta celda de revisi√≥n de recursos despu√©s de que le dijera que iba a usar la GPU L4 de Colab Pro."],"metadata":{"id":"ySq-r8IirbGk"}},{"cell_type":"code","source":["# ============================================================================\n","# CELDA 2: VERIFICACI√ìN DE RECURSOS Y CONFIGURACI√ìN\n","# ============================================================================\n","def check_resources_and_setup():\n","    \"\"\"Verifica recursos y configura par√°metros seguros\"\"\"\n","\n","    print(\"\\n\" + \"=\" * 70)\n","    print(\"üîç VERIFICACI√ìN DE RECURSOS - Google Colab Pro\")\n","    print(\"=\" * 70)\n","\n","    # Informaci√≥n de GPU\n","    gpu_info = {}\n","    if torch.cuda.is_available():\n","        gpu_info['name'] = torch.cuda.get_device_name(0)\n","        gpu_info['memory_gb'] = torch.cuda.get_device_properties(0).total_memory / 1e9\n","        gpu_info['available_memory'] = torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated()\n","\n","        print(f\"üéØ GPU: {gpu_info['name']}\")\n","        print(f\"üíæ Memoria GPU total: {gpu_info['memory_gb']:.2f} GB\")\n","        print(f\"üíæ Memoria GPU disponible: {gpu_info['available_memory'] / 1e9:.2f} GB\")\n","\n","        # Seleccionar modelo seg√∫n GPU\n","        if gpu_info['memory_gb'] >= 16:  # V100/A100\n","            model_name = \"google-t5/t5-base\"  # 220M par√°metros\n","            batch_size = 4  # Reducido para ser conservador\n","            print(\"   ‚Üí Usando T5-base (220M par√°metros) con batch size 4\")\n","        else:  # T4 15GB o menos\n","            model_name = \"google-t5/t5-small\"  # 60M par√°metros\n","            batch_size = 8\n","            print(\"   ‚Üí Usando T5-small (60M par√°metros) con batch size 8\")\n","    else:\n","        print(\"‚ö†Ô∏è  No hay GPU disponible - Usando CPU (muy lento)\")\n","        model_name = \"google-t5/t5-small\"\n","        batch_size = 2\n","\n","    # Informaci√≥n de RAM\n","    import psutil\n","    ram_gb = psutil.virtual_memory().total / 1e9\n","    ram_available = psutil.virtual_memory().available / 1e9\n","    print(f\"\\nüíø RAM Total: {ram_gb:.2f} GB\")\n","    print(f\"üíø RAM Disponible: {ram_available:.2f} GB\")\n","\n","    # Espacio en disco\n","    import shutil\n","    total, used, free = shutil.disk_usage(\"/\")\n","    print(f\"üìÅ Disco total: {total / 1e9:.1f} GB\")\n","    print(f\"üìÅ Disco libre: {free / 1e9:.1f} GB\")\n","\n","    return model_name, batch_size, gpu_info\n","\n","MODEL_NAME, BASE_BATCH_SIZE, GPU_INFO = check_resources_and_setup()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q8dobMqYKUBE","executionInfo":{"status":"ok","timestamp":1770134278011,"user_tz":-60,"elapsed":14,"user":{"displayName":"Guillermo Barrio","userId":"06925154007558381895"}},"outputId":"5c2f2a34-f686-42b3-f01b-6bc282fe3b4a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","======================================================================\n","üîç VERIFICACI√ìN DE RECURSOS - Google Colab Pro\n","======================================================================\n","üéØ GPU: NVIDIA L4\n","üíæ Memoria GPU total: 23.80 GB\n","üíæ Memoria GPU disponible: 23.80 GB\n","   ‚Üí Usando T5-base (220M par√°metros) con batch size 4\n","\n","üíø RAM Total: 56.86 GB\n","üíø RAM Disponible: 54.45 GB\n","üìÅ Disco total: 253.1 GB\n","üìÅ Disco libre: 210.4 GB\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"Z4-SaDOmKT-K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"xVDufvXYKT7J"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Se cofiguran los par√°metros seguros y dem√°s. Es aqu√≠ cuando me di cuenta de lo conveniente que es tener librer√≠as como Unsloth..."],"metadata":{"id":"KFToTJSqr8lt"}},{"cell_type":"code","source":["# ============================================================================\n","# CELDA 3: CONFIGURACI√ìN DE PAR√ÅMETROS SEGUROS\n","# ============================================================================\n","class SafeT5TrainingConfig:\n","    \"\"\"Configuraci√≥n segura para fine-tuning de T5\"\"\"\n","\n","    # ========== MODELO ==========\n","    model_name = MODEL_NAME\n","    tokenizer_name = MODEL_NAME\n","\n","    # ========== DATASET ==========\n","    # Cambia estas rutas seg√∫n tu estructura\n","    dataset_base_path = \"/content/drive/MyDrive/Practica_LLM_Engineering_25/dataset_t5_final\"\n","    train_path = f\"{dataset_base_path}/train.jsonl\"\n","    val_path = f\"{dataset_base_path}/val.jsonl\"\n","    test_path = f\"{dataset_base_path}/test.jsonl\"\n","\n","    # ========== HIPERPAR√ÅMETROS SEGUROS ==========\n","    max_input_length = 256  # Reducido para seguridad\n","    max_target_length = 128\n","    batch_size = BASE_BATCH_SIZE\n","    gradient_accumulation_steps = max(1, 8 // BASE_BATCH_SIZE)  # Ajuste autom√°tico\n","    num_train_epochs = 3\n","    learning_rate = 2e-4  # Un poco m√°s bajo para estabilidad\n","    weight_decay = 0.01\n","    warmup_steps = 50\n","\n","    # ========== ENTRENAMIENTO SEGURO ==========\n","    logging_steps = 25\n","    eval_steps = 100  # Evaluar cada 100 pasos\n","    save_steps = 200\n","    save_total_limit = 2\n","    load_best_model_at_end = True\n","    metric_for_best_model = \"eval_loss\"\n","    greater_is_better = False\n","\n","    # ========== GENERACI√ìN (DESACTIVADA DURANTE EVAL) ==========\n","    num_beams = 4\n","    temperature = 0.7\n","    top_p = 0.9\n","    repetition_penalty = 1.2\n","\n","    # ========== SALIDA ==========\n","    output_dir = f\"/content/drive/MyDrive/Practica_LLM_Engineering_25/t5_electoral_safe_{datetime.now().strftime('%Y%m%d_%H%M')}\"\n","    report_to = \"none\"\n","\n","    # ========== SEGURIDAD ==========\n","    use_safe_tokenization = True  # Usar tokenizaci√≥n segura\n","    filter_problematic_examples = True  # Filtrar ejemplos problem√°ticos\n","    predict_with_generate_during_training = False  # ¬°IMPORTANTE! Desactivar generaci√≥n durante eval\n","\n","    def __str__(self):\n","        config_str = \"\\n‚öôÔ∏è CONFIGURACI√ìN DE ENTRENAMIENTO SEGURO:\\n\"\n","        config_str += \"=\" * 50 + \"\\n\"\n","\n","        for key, value in self.__dict__.items():\n","            if not key.startswith('_'):\n","                config_str += f\"{key:35}: {value}\\n\"\n","\n","        return config_str\n","\n","config = SafeT5TrainingConfig()\n","print(config)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v5-BFsZUKT4R","executionInfo":{"status":"ok","timestamp":1770134282103,"user_tz":-60,"elapsed":13,"user":{"displayName":"Guillermo Barrio","userId":"06925154007558381895"}},"outputId":"d8034955-7541-41eb-bb28-b1845a80bf67"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","‚öôÔ∏è CONFIGURACI√ìN DE ENTRENAMIENTO SEGURO:\n","==================================================\n","\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"hNR8OvbfKT1J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Daz6MMquKTyK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Cargamos los tres datasests de train, test y validaci√≥n, y se muestran una serie de ejemplos."],"metadata":{"id":"FlG5MsLgscxd"}},{"cell_type":"code","source":["# ============================================================================\n","# CELDA 4: CARGA Y VERIFICACI√ìN DEL DATASET\n","# ============================================================================\n","from datasets import Dataset, DatasetDict\n","import hashlib\n","\n","print(\"\\n\" + \"=\" * 70)\n","print(\"üìÇ CARGA Y VERIFICACI√ìN DEL DATASET\")\n","print(\"=\" * 70)\n","\n","def load_and_validate_dataset(config):\n","    \"\"\"Carga y valida el dataset cuidadosamente\"\"\"\n","\n","    def read_jsonl_safely(filepath, max_examples=None):\n","        \"\"\"Lee JSONL con validaci√≥n robusta\"\"\"\n","        data = []\n","        errors = []\n","\n","        try:\n","            with open(filepath, 'r', encoding='utf-8') as f:\n","                for line_num, line in enumerate(f, 1):\n","                    if max_examples and len(data) >= max_examples:\n","                        break\n","\n","                    line = line.strip()\n","                    if not line:\n","                        continue\n","\n","                    try:\n","                        example = json.loads(line)\n","\n","                        # Validar estructura\n","                        if not isinstance(example, dict):\n","                            errors.append(f\"L√≠nea {line_num}: No es un diccionario\")\n","                            continue\n","\n","                        # Verificar campos requeridos\n","                        if 'input_text' not in example or 'target_text' not in example:\n","                            errors.append(f\"L√≠nea {line_num}: Faltan campos requeridos\")\n","                            continue\n","\n","                        # Verificar que no est√©n vac√≠os\n","                        if not example['input_text'] or not example['target_text']:\n","                            errors.append(f\"L√≠nea {line_num}: Campos vac√≠os\")\n","                            continue\n","\n","                        # Limpiar textos\n","                        input_text = str(example['input_text']).strip()\n","                        target_text = str(example['target_text']).strip()\n","\n","                        # A√±adir ID √∫nico\n","                        _id = hashlib.md5(\n","                            f\"{input_text}_{target_text}\".encode()\n","                        ).hexdigest()[:8]\n","\n","                        # Only append the cleaned and necessary fields to ensure consistent schema\n","                        data.append({\n","                            'input_text': input_text,\n","                            'target_text': target_text,\n","                            '_id': _id\n","                        })\n","\n","                    except json.JSONDecodeError as e:\n","                        errors.append(f\"L√≠nea {line_num}: JSON inv√°lido - {str(e)}\")\n","                    except Exception as e:\n","                        errors.append(f\"L√≠nea {line_num}: Error - {str(e)}\")\n","\n","            print(f\"  ‚úÖ {len(data)} ejemplos v√°lidos\")\n","            if errors:\n","                print(f\"  ‚ö†Ô∏è  {len(errors)} errores (primeros 3):\")\n","                for err in errors[:3]:\n","                    print(f\"     {err}\")\n","\n","            return data\n","\n","        except FileNotFoundError:\n","            print(f\"  ‚ùå Archivo no encontrado: {filepath}\")\n","            return []\n","        except Exception as e:\n","            print(f\"  ‚ùå Error cargando {filepath}: {e}\")\n","            return []\n","\n","    print(\"Cargando splits...\")\n","\n","    # Cargar con l√≠mite si es muy grande\n","    train_data = read_jsonl_safely(config.train_path)\n","    val_data = read_jsonl_safely(config.val_path) if os.path.exists(config.val_path) else []\n","    test_data = read_jsonl_safely(config.test_path) if os.path.exists(config.test_path) else []\n","\n","    if not train_data:\n","        raise ValueError(\"‚ùå No se pudo cargar el dataset de entrenamiento\")\n","\n","    # Crear DatasetDict\n","    dataset_dict = {}\n","    dataset_dict[\"train\"] = Dataset.from_list(train_data)\n","\n","    if val_data:\n","        dataset_dict[\"validation\"] = Dataset.from_list(val_data)\n","\n","    if test_data:\n","        dataset_dict[\"test\"] = Dataset.from_list(test_data)\n","\n","    dataset_dict = DatasetDict(dataset_dict)\n","\n","    # Estad√≠sticas\n","    print(f\"\\nüìä ESTAD√çSTICAS DEL DATASET:\")\n","    print(f\"  Entrenamiento: {len(train_data)} ejemplos\")\n","    if val_data:\n","        print(f\"  Validaci√≥n: {len(val_data)} ejemplos\")\n","    if test_data:\n","        print(f\"  Test: {len(test_data)} ejemplos\")\n","\n","    # Mostrar ejemplos de muestra\n","    print(f\"\\nüìù MUESTRA DEL DATASET (3 ejemplos):\")\n","    for i in range(min(3, len(train_data))):\n","        example = train_data[i]\n","        print(f\"\\n  Ejemplo {i+1} (ID: {example.get('_id', 'N/A')}):\")\n","        print(f\"  Input: {example['input_text'][:80]}...\")\n","        print(f\"  Target: {example['target_text'][:80]}...\")\n","        if 'metadata' in example:\n","            nivel = example['metadata'].get('nivel_original', 'N/A')\n","            dificultad = example['metadata'].get('dificultad', 'N/A')\n","            print(f\"  Nivel: {nivel}, Dificultad: {dificultad}\")\n","\n","    return dataset_dict\n","\n","# Cargar dataset\n","dataset = load_and_validate_dataset(config)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hhm2Tq7UKTsZ","executionInfo":{"status":"ok","timestamp":1770134286739,"user_tz":-60,"elapsed":505,"user":{"displayName":"Guillermo Barrio","userId":"06925154007558381895"}},"outputId":"665b55d0-2d21-4c4d-c147-91304c86be28"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","======================================================================\n","üìÇ CARGA Y VERIFICACI√ìN DEL DATASET\n","======================================================================\n","Cargando splits...\n","  ‚úÖ 1440 ejemplos v√°lidos\n","  ‚úÖ 180 ejemplos v√°lidos\n","  ‚úÖ 180 ejemplos v√°lidos\n","\n","üìä ESTAD√çSTICAS DEL DATASET:\n","  Entrenamiento: 1440 ejemplos\n","  Validaci√≥n: 180 ejemplos\n","  Test: 180 ejemplos\n","\n","üìù MUESTRA DEL DATASET (3 ejemplos):\n","\n","  Ejemplo 1 (ID: 1863b614):\n","  Input: ### Instrucci√≥n:\n","¬øCu√°l fue el % VOX mediano en la comunidad de Andaluc√≠a en Novi...\n","  Target: El % VOX mediano en la comunidad de Andaluc√≠a en Noviembre 2019 fue de 19.90%....\n","\n","  Ejemplo 2 (ID: 86113f39):\n","  Input: ### Instrucci√≥n:\n","¬øQu√© diferencia hay entre PP y VOX en la provincia de Segovia e...\n","  Target: En la elecci√≥n de Abril 2019 en la provincia de Segovia, el partido PP obtuvo el...\n","\n","  Ejemplo 3 (ID: 2fcd4bb0):\n","  Input: ### Instrucci√≥n:\n","¬øC√≥mo evolucion√≥ el porcentaje de voto a VOX en Cillorigo de Li...\n","  Target: El VOX en Cillorigo de Li√©bana pas√≥ de 19.61% en Noviembre 2019 a 15.03% en Abri...\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"0VTqEJnlKTph"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"IKFCdiaFtQKf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Llegamos a la tokenizacion que, pese a cargar el tokenizador desde transformers, tuve continuos problemas con √©l. Tras las rectificaciones de DeepSeek, Colab estuvo cerca de una hora intentando solucionar el problema."],"metadata":{"id":"5sOuDVE9tQtH"}},{"cell_type":"code","source":["# ============================================================================\n","# CELDA 5: TOKENIZACI√ìN SEGURA PARA T5\n","# ============================================================================\n","from transformers import T5Tokenizer\n","import re\n","\n","print(\"\\n\" + \"=\" * 70)\n","print(\"üî§ TOKENIZACI√ìN SEGURA DEL DATASET\")\n","print(\"=\" * 70)\n","\n","print(f\"Cargando tokenizer: {config.tokenizer_name}\")\n","tokenizer = T5Tokenizer.from_pretrained(config.tokenizer_name)\n","\n","# Configurar tokenizer para T5\n","tokenizer.pad_token = tokenizer.eos_token\n","\n","print(f\"  Vocabulario: {tokenizer.vocab_size:,} tokens\")\n","print(f\"  Pad token: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})\")\n","print(f\"  EOS token: {tokenizer.eos_token} (ID: {tokenizer.eos_token_id})\")\n","\n","# ============================================================================\n","# FUNCI√ìN DE PREPROCESAMIENTO SEGURA\n","# ============================================================================\n","def safe_preprocess_function(examples, is_validation=False):\n","    \"\"\"\n","    Funci√≥n de preprocesamiento SEGURA con verificaci√≥n exhaustiva\n","    \"\"\"\n","    # Limpiar textos\n","    def clean_text(text):\n","        if not isinstance(text, str):\n","            return \"\"\n","\n","        # 1. Normalizar espacios\n","        text = ' '.join(text.split())\n","\n","        # 2. Mantener solo caracteres seguros\n","        # Permitir: letras, n√∫meros, espacios, puntuaci√≥n b√°sica, acentos espa√±oles\n","        safe_pattern = r'[^a-zA-Z0-9√°√©√≠√≥√∫√Å√â√ç√ì√ö√±√ë√º√ú\\s.,;:¬ø?¬°!()\\-%\\d]'\n","        text = re.sub(safe_pattern, '', text)\n","\n","        # 3. Limitar longitud\n","        max_chars = config.max_input_length * 4  # Estimaci√≥n conservadora\n","        if len(text) > max_chars:\n","            text = text[:max_chars]\n","\n","        return text.strip()\n","\n","    # Limpiar inputs y targets\n","    inputs = [clean_text(text) for text in examples[\"input_text\"]]\n","    targets = [clean_text(text) for text in examples[\"target_text\"]]\n","\n","    # Tokenizar inputs\n","    model_inputs = tokenizer(\n","        inputs,\n","        max_length=config.max_input_length,\n","        truncation=True,\n","        padding=\"max_length\",\n","        return_tensors=None\n","    )\n","\n","    # Tokenizar targets con contexto de target tokenizer\n","    with tokenizer.as_target_tokenizer():\n","        tokenized_targets = tokenizer(\n","            targets,\n","            max_length=config.max_target_length,\n","            truncation=True,\n","            padding=\"max_length\",\n","            return_tensors=None\n","        )\n","\n","    # VERIFICACI√ìN Y CORRECCI√ìN EXHAUSTIVA DE LABELS\n","    labels = tokenized_targets[\"input_ids\"]\n","\n","    for i in range(len(labels)):\n","        for j in range(len(labels[i])):\n","            token_id = labels[i][j]\n","\n","            # 1. Verificar que sea un n√∫mero entero\n","            if not isinstance(token_id, int):\n","                labels[i][j] = tokenizer.pad_token_id\n","                continue\n","\n","            # 2. Verificar rango del vocabulario (0 a vocab_size-1)\n","            if token_id < 0 or token_id >= tokenizer.vocab_size:\n","                labels[i][j] = tokenizer.pad_token_id\n","                continue\n","\n","            # 3. Reemplazar padding token por -100 (ignore index)\n","            if token_id == tokenizer.pad_token_id:\n","                labels[i][j] = -100\n","\n","    model_inputs[\"labels\"] = labels\n","\n","    # Verificaci√≥n de seguridad (solo para primeros ejemplos)\n","    if not is_validation and len(model_inputs[\"input_ids\"]) > 0:\n","        # Verificar primer ejemplo\n","        first_input_ids = model_inputs[\"input_ids\"][0]\n","        first_labels = model_inputs[\"labels\"][0]\n","\n","        max_input_id = max(first_input_ids)\n","        valid_labels = [l for l in first_labels if l != -100]\n","        max_label_id = max(valid_labels) if valid_labels else 0\n","\n","        if max_input_id >= tokenizer.vocab_size:\n","            print(f\"‚ö†Ô∏è  ADVERTENCIA: input_ids tiene ID {max_input_id} >= {tokenizer.vocab_size}\")\n","\n","        if valid_labels and max_label_id >= tokenizer.vocab_size:\n","            print(f\"‚ö†Ô∏è  ADVERTENCIA: labels tiene ID {max_label_id} >= {tokenizer.vocab_size}\")\n","\n","    return model_inputs\n","\n","# ============================================================================\n","# DIAGN√ìSTICO DE PROBLEMAS EN VALIDATION SET\n","# ============================================================================\n","print(\"\\n\" + \"=\" * 70)\n","print(\"üîç DIAGN√ìSTICO PREVENTIVO DE VALIDATION SET\")\n","print(\"=\" * 70)\n","\n","def diagnose_validation_set(validation_dataset):\n","    \"\"\"Diagnostica y corrige problemas en el validation set\"\"\"\n","\n","    if not validation_dataset:\n","        print(\"‚ö†Ô∏è  No hay validation set para diagnosticar\")\n","        return None\n","\n","    print(f\"Analizando validation set ({len(validation_dataset)} ejemplos)...\")\n","\n","    # Tokenizar validation set para diagn√≥stico\n","    val_tokenized_for_diagnosis = validation_dataset.map(\n","        lambda x: safe_preprocess_function(x, is_validation=True),\n","        batched=True,\n","        batch_size=32,\n","        remove_columns=validation_dataset.column_names\n","    )\n","\n","    # Buscar ejemplos problem√°ticos\n","    problematic_indices = []\n","    problematic_details = []\n","\n","    for i in range(len(val_tokenized_for_diagnosis)):\n","        example = val_tokenized_for_diagnosis[i]\n","\n","        # Verificar input_ids\n","        input_ids = example[\"input_ids\"]\n","        if any(token_id >= tokenizer.vocab_size for token_id in input_ids):\n","            problematic_indices.append(i)\n","            problematic_details.append({\n","                \"index\": i,\n","                \"issue\": \"input_ids out of range\",\n","                \"max_id\": max(input_ids),\n","                \"vocab_size\": tokenizer.vocab_size\n","            })\n","            continue\n","\n","        # Verificar labels\n","        labels = example[\"labels\"]\n","        valid_labels = [l for l in labels if l != -100]\n","\n","        if valid_labels:\n","            if any(label_id >= tokenizer.vocab_size for label_id in valid_labels):\n","                problematic_indices.append(i)\n","                problematic_details.append({\n","                    \"index\": i,\n","                    \"issue\": \"labels out of range\",\n","                    \"max_id\": max(valid_labels),\n","                    \"vocab_size\": tokenizer.vocab_size\n","                })\n","\n","    if problematic_indices:\n","        print(f\"‚ùå ENCONTRADOS {len(problematic_indices)} EJEMPLOS PROBLEM√ÅTICOS\")\n","        print(\"Detalles (primeros 5):\")\n","        for detail in problematic_details[:5]:\n","            print(f\"  √çndice {detail['index']}: {detail['issue']}\")\n","            print(f\"     ID m√°ximo: {detail['max_id']}, Vocab size: {detail['vocab_size']}\")\n","\n","        # Mostrar textos problem√°ticos\n","        print(f\"\\nüìù Textos de ejemplos problem√°ticos:\")\n","        for idx in problematic_indices[:3]:\n","            original_example = validation_dataset[idx]\n","            print(f\"\\n  √çndice {idx}:\")\n","            print(f\"  Input: {original_example['input_text'][:100]}...\")\n","            print(f\"  Target: {original_example['target_text'][:100]}...\")\n","\n","        if config.filter_problematic_examples:\n","            print(f\"\\nüõ†Ô∏è  FILTRANDO EJEMPLOS PROBLEM√ÅTICOS...\")\n","\n","            # Crear nuevo validation set sin ejemplos problem√°ticos\n","            good_indices = [i for i in range(len(validation_dataset)) if i not in problematic_indices]\n","\n","            from datasets import Dataset\n","            good_examples = [validation_dataset[i] for i in good_indices]\n","\n","            print(f\"  Original: {len(validation_dataset)} ejemplos\")\n","            print(f\"  Filtrado: {len(good_examples)} ejemplos\")\n","            print(f\"  Eliminados: {len(problematic_indices)} ejemplos\")\n","\n","            return Dataset.from_list(good_examples)\n","        else:\n","            print(\"‚ö†Ô∏è  Manteniendo ejemplos problem√°ticos (puede causar errores)\")\n","            return validation_dataset\n","    else:\n","        print(\"‚úÖ Validation set verificado - No se encontraron problemas\")\n","        return validation_dataset\n","\n","# Aplicar diagn√≥stico al validation set si existe\n","if \"validation\" in dataset:\n","    print(\"\\nAplicando diagn√≥stico al validation set...\")\n","    dataset[\"validation\"] = diagnose_validation_set(dataset[\"validation\"])\n","\n","# ============================================================================\n","# TOKENIZACI√ìN FINAL\n","# ============================================================================\n","print(\"\\nüîÑ TOKENIZACI√ìN FINAL DE DATASETS...\")\n","\n","def safe_tokenize_dataset(dataset_split, split_name):\n","    \"\"\"Tokeniza un split del dataset de forma segura\"\"\"\n","    print(f\"  Tokenizando {split_name}...\")\n","\n","    is_validation = (split_name == \"validation\")\n","\n","    tokenized = dataset_split.map(\n","        lambda x: safe_preprocess_function(x, is_validation=is_validation),\n","        batched=True,\n","        batch_size=32,\n","        remove_columns=dataset_split.column_names\n","    )\n","\n","    print(f\"    ‚úÖ {len(tokenized)} ejemplos tokenizados\")\n","    return tokenized\n","\n","tokenized_datasets = {}\n","\n","# Tokenizar train\n","tokenized_datasets[\"train\"] = safe_tokenize_dataset(dataset[\"train\"], \"train\")\n","\n","# Tokenizar validation si existe y no est√° vac√≠o\n","if \"validation\" in dataset and len(dataset[\"validation\"]) > 0:\n","    tokenized_datasets[\"validation\"] = safe_tokenize_dataset(dataset[\"validation\"], \"validation\")\n","else:\n","    print(\"‚ö†Ô∏è  Validation set no disponible o vac√≠o\")\n","    config.eval_steps = None  # Desactivar evaluaci√≥n\n","\n","# Tokenizar test si existe\n","if \"test\" in dataset and len(dataset[\"test\"]) > 0:\n","    tokenized_datasets[\"test\"] = safe_tokenize_dataset(dataset[\"test\"], \"test\")\n","\n","print(f\"\\n‚úÖ TOKENIZACI√ìN COMPLETADA:\")\n","print(f\"  Train: {len(tokenized_datasets['train'])} ejemplos\")\n","if \"validation\" in tokenized_datasets:\n","    print(f\"  Validation: {len(tokenized_datasets['validation'])} ejemplos\")\n","if \"test\" in tokenized_datasets:\n","    print(f\"  Test: {len(tokenized_datasets['test'])} ejemplos\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":703,"referenced_widgets":["66e15b43f11f43c8a34e87cd1dd53e16","9a4fc2a26d274e7e99976a36e80d7755","95f5a828833e4e9890e665451550705a","13bafed626e84e58bf0cead01dd48789","c4e5fed8ecfd42fd97bcd09cf730ff29","215ccd2b0b48468dac12712ad5995b64","7549c916e1dd460ab9846cebe977921e","ff89664c6e8c4ef89eb047eb01d72833","cd9a4b4a3c074d98b04b6c563ef4af09","d5687f0a4fdc470bb15c2a57684190e2","a5328e4b8f9d42f7b133de04064e44f4","e7ca88f02179402f9ffd1f29a270e722","ea88385680c64d7a821b100cf6d676e8","79544125cb454f04955faa2b5d5f995e","b1f20ccb710f49c6b01d1ec9bb10b4f7","95eafa55b6c1446bb544186cb760a36d","d120ac3dbf78496691da2155d428d1d8","c3b9c903fc344603abf0346b6edb9335","0cd47509ba1b47e2ad333dd635445909","16ce3bc5edf24f8f962ba3618bf75f70","9c5019afe3e546f48334072e57dce64a","af9882eeb0554e17b5f5b0685b492f58","e69f8f50f33e42d089018cf2a80430b3","7d1ee7b85dcf4921aaccee7697666f1e","79701a682f3145fc95c748bdaa0f1c34","2f5b8fe006c94dca8c9af24131453a2f","39c85797b6f444c396068c62e99dc038","355b9e871d1343edb88861a5b0d49994","79438f09899445749d53311bb5bb0589","cf2e73f829164f3fb686d3cd71c5e078","23f6a8f6ca14402fba7a2ec9860acd43","2824a7db603f4fbda6f17d2eb5273a16","9db9e040ef054a978af2e5dfbb3cc804","d66069a13e1c4403ae03049e4fe9a279","3a1fe2d8058d401c9f71fc3ee8324771","b952abe1968140dea536d00b003f4183","fde1ca3e5136410cb151887ca5c06098","13c4e82a47094a02a8005731f8dd9835","a34c2925eddb4062828507e92e3565fc","f6e84d95490e4cbb92e2a0e733331b06","ef3879e322954a01a885714811e1169e","f8a9ea8386da4148bb26e90de8192bad","e10db449a6284d6d9a3deeda1214883c","363da69fa6b241d1840de57707debc20"]},"id":"14CGiENcKTmp","executionInfo":{"status":"ok","timestamp":1770134295360,"user_tz":-60,"elapsed":3293,"user":{"displayName":"Guillermo Barrio","userId":"06925154007558381895"}},"outputId":"4ffc2cf7-5d57-4984-d9f4-820501b73c33"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","======================================================================\n","üî§ TOKENIZACI√ìN SEGURA DEL DATASET\n","======================================================================\n","Cargando tokenizer: google-t5/t5-base\n"]},{"output_type":"stream","name":"stderr","text":["You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"output_type":"stream","name":"stdout","text":["  Vocabulario: 32,000 tokens\n","  Pad token: </s> (ID: 1)\n","  EOS token: </s> (ID: 1)\n","\n","======================================================================\n","üîç DIAGN√ìSTICO PREVENTIVO DE VALIDATION SET\n","======================================================================\n","\n","Aplicando diagn√≥stico al validation set...\n","Analizando validation set (180 ejemplos)...\n"]},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/180 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66e15b43f11f43c8a34e87cd1dd53e16"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["‚úÖ Validation set verificado - No se encontraron problemas\n","\n","üîÑ TOKENIZACI√ìN FINAL DE DATASETS...\n","  Tokenizando train...\n"]},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/1440 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7ca88f02179402f9ffd1f29a270e722"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["    ‚úÖ 1440 ejemplos tokenizados\n","  Tokenizando validation...\n"]},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/180 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e69f8f50f33e42d089018cf2a80430b3"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["    ‚úÖ 180 ejemplos tokenizados\n","  Tokenizando test...\n"]},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/180 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d66069a13e1c4403ae03049e4fe9a279"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["    ‚úÖ 180 ejemplos tokenizados\n","\n","‚úÖ TOKENIZACI√ìN COMPLETADA:\n","  Train: 1440 ejemplos\n","  Validation: 180 ejemplos\n","  Test: 180 ejemplos\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"93wSipWiKTjh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"63BKgE0lKTg5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Cargamos el modelo T5 desde transformers, esta celda fue quiz√°s la √∫nica que no dio problemas."],"metadata":{"id":"wuPyX4akuBQg"}},{"cell_type":"code","source":["# ============================================================================\n","# CELDA 6: CARGA DEL MODELO T5\n","# ============================================================================\n","from transformers import T5ForConditionalGeneration\n","\n","print(\"\\n\" + \"=\" * 70)\n","print(\"üèóÔ∏è  CARGA DEL MODELO T5\")\n","print(\"=\" * 70)\n","\n","print(f\"Cargando modelo: {config.model_name}\")\n","\n","try:\n","    # Configuraci√≥n del modelo\n","    model = T5ForConditionalGeneration.from_pretrained(config.model_name)\n","\n","    # Mover a GPU si est√° disponible\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model = model.to(device)\n","\n","    print(f\"‚úÖ Modelo cargado en: {device}\")\n","    print(f\"   Par√°metros totales: {sum(p.numel() for p in model.parameters()):,}\")\n","    print(f\"   Par√°metros entrenables: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n","\n","except Exception as e:\n","    print(f\"‚ùå Error cargando el modelo: {e}\")\n","    print(\"Intentando cargar versi√≥n alternativa...\")\n","\n","    # Intentar con configuraciones alternativas\n","    try:\n","        model = T5ForConditionalGeneration.from_pretrained(\n","            config.model_name,\n","            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n","        )\n","        model = model.to(device)\n","        print(\"‚úÖ Modelo cargado con configuraci√≥n alternativa\")\n","    except Exception as e2:\n","        print(f\"‚ùå Error cr√≠tico: {e2}\")\n","        raise\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gQFH6Q9VKTd7","executionInfo":{"status":"ok","timestamp":1770134313524,"user_tz":-60,"elapsed":9773,"user":{"displayName":"Guillermo Barrio","userId":"06925154007558381895"}},"outputId":"788700cd-45a6-4f03-9713-e3d9c77626d6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","======================================================================\n","üèóÔ∏è  CARGA DEL MODELO T5\n","======================================================================\n","Cargando modelo: google-t5/t5-base\n","‚úÖ Modelo cargado en: cuda\n","   Par√°metros totales: 222,903,552\n","   Par√°metros entrenables: 222,903,552\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"93fZk5SAKTa5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"EAAF8G8LKTXx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Esta celda configura las m√©tricas, de forma 'segura'. Esto de 'seguro' es una constante de DeepSeek, tras informarle yo de los problemas que iba encontrando.\n","\n","De todas formas, soy algo esc√©ptico, quiz√°s debido a mi ignorancia, con esta mediciones de Rouge, al menos en esta pr√°ctica. Lo explico en el pdf de presentaci√≥n de la pr√°ctica."],"metadata":{"id":"6XNxZa28uNjY"}},{"cell_type":"code","source":["# ============================================================================\n","# CELDA 7: CONFIGURACI√ìN DE M√âTRICAS SEGURAS\n","# ============================================================================\n","import evaluate\n","import nltk\n","\n","print(\"\\n\" + \"=\" * 70)\n","print(\"üìä CONFIGURACI√ìN DE M√âTRICAS SEGURAS\")\n","print(\"=\" * 70)\n","\n","try:\n","    nltk.download('punkt', quiet=True)\n","\n","    # Cargar m√©tricas\n","    rouge = evaluate.load(\"rouge\")\n","\n","    def safe_compute_metrics(eval_pred):\n","        \"\"\"\n","        Calcula m√©tricas de forma segura\n","        Solo se usar√° si predict_with_generate=True\n","        \"\"\"\n","        predictions, labels = eval_pred\n","\n","        # Verificar que las predicciones sean v√°lidas\n","        if predictions is None or labels is None:\n","            return {\"eval_loss\": 0.0}\n","\n","        # Decodificar predicciones\n","        try:\n","            decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n","        except:\n","            decoded_preds = [\"\"] * len(predictions)\n","\n","        # Reemplazar -100 en labels\n","        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n","        try:\n","            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n","        except:\n","            decoded_labels = [\"\"] * len(labels)\n","\n","        # Calcular ROUGE de forma segura\n","        try:\n","            rouge_result = rouge.compute(\n","                predictions=decoded_preds,\n","                references=decoded_labels,\n","                use_stemmer=True,\n","                use_aggregator=True\n","            )\n","        except:\n","            rouge_result = {\"rouge1\": 0.0, \"rouge2\": 0.0, \"rougeL\": 0.0, \"rougeLsum\": 0.0}\n","\n","        return {\n","            \"rouge1\": rouge_result[\"rouge1\"],\n","            \"rouge2\": rouge_result[\"rouge2\"],\n","            \"rougeL\": rouge_result[\"rougeL\"],\n","            \"rougeLsum\": rouge_result[\"rougeLsum\"]\n","        }\n","\n","    print(\"‚úÖ M√©tricas configuradas (solo para evaluaci√≥n final)\")\n","\n","except Exception as e:\n","    print(f\"‚ö†Ô∏è  Error configurando m√©tricas: {e}\")\n","    print(\"Continuando sin m√©tricas detalladas\")\n","\n","    def safe_compute_metrics(eval_pred):\n","        return {\"eval_loss\": 0.0}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SJnreGdUKTU5","executionInfo":{"status":"ok","timestamp":1770134317989,"user_tz":-60,"elapsed":1288,"user":{"displayName":"Guillermo Barrio","userId":"06925154007558381895"}},"outputId":"3f4ee787-0fd9-4c01-c175-8652321bb425"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","======================================================================\n","üìä CONFIGURACI√ìN DE M√âTRICAS SEGURAS\n","======================================================================\n","‚úÖ M√©tricas configuradas (solo para evaluaci√≥n final)\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"gDcNez12KTSK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Ahora configuramos el finetuning, de nuevo de forma 'segura', con la librer√≠a de transformers. La mayor parte de los par√°metros los hemos definido en la celda 3, con el objeto creado all√≠ de la clase SafeT5TrainingConfig, llamado 'config'.\n","\n","Por fin, definimos el trainer con la funci√≥n Seq2SeqTrainer()."],"metadata":{"id":"Q7aObg5YwmFk"}},{"cell_type":"code","source":["# ============================================================================\n","# CELDA 8: CONFIGURACI√ìN DEL ENTRENAMIENTO SEGURO\n","# ============================================================================\n","from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n","\n","print(\"\\n\" + \"=\" * 70)\n","print(\"üéØ CONFIGURACI√ìN DEL ENTRENAMIENTO SEGURO\")\n","print(\"=\" * 70)\n","\n","# Data collator para T5\n","data_collator = DataCollatorForSeq2Seq(\n","    tokenizer=tokenizer,\n","    model=model,\n","    padding=True\n",")\n","\n","# Configurar si hay validation set\n","has_validation = \"validation\" in tokenized_datasets and len(tokenized_datasets[\"validation\"]) > 0\n","\n","# Argumentos de entrenamiento SEGUROS\n","training_args = Seq2SeqTrainingArguments(\n","    output_dir=config.output_dir,\n","    overwrite_output_dir=True,\n","\n","    # Hiperpar√°metros\n","    num_train_epochs=config.num_train_epochs,\n","    per_device_train_batch_size=config.batch_size,\n","    per_device_eval_batch_size=config.batch_size,\n","    gradient_accumulation_steps=config.gradient_accumulation_steps,\n","    learning_rate=config.learning_rate,\n","    weight_decay=config.weight_decay,\n","    warmup_steps=config.warmup_steps,\n","\n","    # Estrategias SEGURAS\n","    evaluation_strategy=\"steps\" if has_validation else \"no\",\n","    eval_steps=config.eval_steps if has_validation else None,\n","    save_strategy=\"steps\",\n","    save_steps=config.save_steps,\n","    save_total_limit=config.save_total_limit,\n","    load_best_model_at_end=has_validation,\n","    metric_for_best_model=config.metric_for_best_model if has_validation else None,\n","    greater_is_better=config.greater_is_better if has_validation else None,\n","\n","    # Logging y reporte\n","    logging_strategy=\"steps\",\n","    logging_steps=config.logging_steps,\n","    report_to=config.report_to,\n","\n","    # Optimizaci√≥n SEGURA\n","    fp16=torch.cuda.is_available(),\n","    optim=\"adafactor\",  # Adafactor es m√°s estable para T5\n","    gradient_checkpointing=True,\n","\n","    # ‚ö†Ô∏è CONFIGURACI√ìN CR√çTICA: Generaci√≥n durante evaluaci√≥n\n","    # Desactivar para evitar error \"piece id is out of range\"\n","    predict_with_generate=config.predict_with_generate_during_training and has_validation,\n","\n","    # Configuraci√≥n de generaci√≥n (solo si predict_with_generate=True)\n","    generation_max_length=config.max_target_length if config.predict_with_generate_during_training else None,\n","    generation_num_beams=config.num_beams if config.predict_with_generate_during_training else None,\n","\n","    # Otros ajustes de seguridad\n","    dataloader_num_workers=0,  # 0 para evitar problemas en Colab\n","    remove_unused_columns=True,\n","    push_to_hub=False,\n","    ddp_find_unused_parameters=False,\n",")\n","\n","print(\"‚úÖ Training arguments configurados SEGUROS\")\n","print(f\"   Output directory: {config.output_dir}\")\n","print(f\"   Batch size: {config.batch_size}\")\n","print(f\"   Gradient accumulation: {config.gradient_accumulation_steps}\")\n","print(f\"   Total effective batch: {config.batch_size * config.gradient_accumulation_steps}\")\n","print(f\"   Evaluation: {'Activada' if has_validation else 'Desactivada'}\")\n","print(f\"   Predict with generate: {training_args.predict_with_generate}\")\n","\n","# Crear trainer SEGURO\n","trainer = Seq2SeqTrainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_datasets[\"train\"],\n","    eval_dataset=tokenized_datasets.get(\"validation\", None),\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n","    compute_metrics=safe_compute_metrics if training_args.predict_with_generate else None,\n",")\n","\n","print(\"‚úÖ Trainer creado exitosamente\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4y84JNAyKTPE","executionInfo":{"status":"ok","timestamp":1770134322308,"user_tz":-60,"elapsed":50,"user":{"displayName":"Guillermo Barrio","userId":"06925154007558381895"}},"outputId":"c9afc0c8-d5c6-4f60-d253-c8963bd90460"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","======================================================================\n","üéØ CONFIGURACI√ìN DEL ENTRENAMIENTO SEGURO\n","======================================================================\n","‚úÖ Training arguments configurados SEGUROS\n","   Output directory: /content/drive/MyDrive/Practica_LLM_Engineering_25/t5_electoral_safe_20260203_1558\n","   Batch size: 4\n","   Gradient accumulation: 2\n","   Total effective batch: 8\n","   Evaluation: Activada\n","   Predict with generate: False\n","‚úÖ Trainer creado exitosamente\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"UB270Wa2KTMa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"375A1w3iKTJT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Ejecutamos el finetuning, que dur√≥ unos 5-7 minutos. Lo cierto es que notamos que los errores van descendiendo a lo largo de las tres √©poca, tanto en train como validaci√≥n."],"metadata":{"id":"iCV2tBcrxuSt"}},{"cell_type":"code","source":["# ============================================================================\n","# CELDA 9: ENTRENAMIENTO SEGURO CON MANEJO DE ERRORES\n","# ============================================================================\n","print(\"\\n\" + \"=\" * 70)\n","print(\"üöÄ INICIANDO ENTRENAMIENTO SEGURO\")\n","print(\"=\" * 70)\n","\n","# Limpiar memoria antes de empezar\n","torch.cuda.empty_cache()\n","gc.collect()\n","\n","# Mostrar memoria disponible\n","if torch.cuda.is_available():\n","    allocated = torch.cuda.memory_allocated() / 1e9\n","    total = torch.cuda.get_device_properties(0).total_memory / 1e9\n","    print(f\"Memoria GPU antes del entrenamiento: {allocated:.2f} GB / {total:.2f} GB\")\n","\n","# Calcular pasos totales estimados\n","total_steps = len(tokenized_datasets[\"train\"]) // (config.batch_size * config.gradient_accumulation_steps) * config.num_train_epochs\n","print(f\"Pasos totales estimados: {total_steps}\")\n","print(f\"Duraci√≥n estimada: {total_steps * 2 / 60:.1f} minutos (aproximadamente)\")\n","\n","# Funci√≥n de callback para manejar errores\n","from transformers import TrainerCallback\n","\n","class SafeTrainingCallback(TrainerCallback):\n","    \"\"\"Callback para manejar errores durante el entrenamiento\"\"\"\n","\n","    def on_step_end(self, args, state, control, **kwargs):\n","        # Verificar cada 50 pasos\n","        if state.global_step % 50 == 0:\n","            torch.cuda.empty_cache()\n","\n","        return control\n","\n","    def on_evaluate(self, args, state, control, **kwargs):\n","        print(f\"\\nüîç Evaluaci√≥n completada en paso {state.global_step}\")\n","        return control\n","\n","# A√±adir callback al trainer\n","trainer.add_callback(SafeTrainingCallback())\n","\n","# INTENTAR ENTRENAR CON MANEJO DE ERRORES\n","try:\n","    print(\"\\n\" + \"=\" * 70)\n","    print(\"‚è≥ ENTRENANDO... (esto puede tomar varios minutos)\")\n","    print(\"=\" * 70)\n","\n","    # Entrenar\n","    train_result = trainer.train()\n","\n","    # Guardar modelo final\n","    trainer.save_model()\n","    tokenizer.save_pretrained(config.output_dir)\n","\n","    # Guardar m√©tricas de entrenamiento\n","    metrics = train_result.metrics\n","    trainer.log_metrics(\"train\", metrics)\n","    trainer.save_metrics(\"train\", metrics)\n","    trainer.save_state()\n","\n","    print(f\"\\n\" + \"=\" * 70)\n","    print(\"üéâ ¬°ENTRENAMIENTO COMPLETADO EXITOSAMENTE!\")\n","    print(\"=\" * 70)\n","    print(f\"üìÅ Modelo guardado en: {config.output_dir}\")\n","\n","    # Mostrar resumen\n","    print(f\"\\nüìä RESUMEN DEL ENTRENAMIENTO:\")\n","    print(f\"  √âpocas completadas: {config.num_train_epochs}\")\n","    print(f\"  Pasos totales: {train_result.global_step}\")\n","    print(f\"  P√©rdida final: {train_result.training_loss:.4f}\")\n","\n","    if has_validation:\n","        print(f\"  P√©rdida en validaci√≥n: {trainer.state.log_history[-1].get('eval_loss', 'N/A')}\")\n","\n","except RuntimeError as e:\n","    if \"out of memory\" in str(e):\n","        print(\"\\n‚ùå ERROR: Memoria insuficiente (OOM)\")\n","        print(\"Intentando recuperaci√≥n...\")\n","\n","        # Guardar lo que se haya logrado\n","        try:\n","            trainer.save_model(config.output_dir + \"_partial\")\n","            print(f\"‚úÖ Modelo parcial guardado en: {config.output_dir}_partial\")\n","        except:\n","            print(\"‚ö†Ô∏è  No se pudo guardar modelo parcial\")\n","\n","        print(\"\\nüí° SOLUCIONES:\")\n","        print(\"1. Reduce batch_size a la mitad\")\n","        print(\"2. Reduce max_input_length y max_target_length\")\n","        print(\"3. Usa gradient_checkpointing=True\")\n","        print(\"4. Usa T5-small en lugar de T5-base\")\n","\n","    elif \"piece id is out of range\" in str(e):\n","        print(\"\\n‚ùå ERROR: Token ID fuera de rango durante evaluaci√≥n\")\n","        print(\"Esto probablemente ocurri√≥ durante la evaluaci√≥n con generaci√≥n.\")\n","\n","        print(\"\\nüõ†Ô∏è  SOLUCI√ìN APLICADA AUTOM√ÅTICAMENTE:\")\n","        print(\"Reentrenando SIN generaci√≥n durante evaluaci√≥n...\")\n","\n","        # Reconfigurar para entrenar sin generaci√≥n\n","        training_args.predict_with_generate = False\n","\n","        # Crear nuevo trainer\n","        trainer_no_generate = Seq2SeqTrainer(\n","            model=model,\n","            args=training_args,\n","            train_dataset=tokenized_datasets[\"train\"],\n","            eval_dataset=tokenized_datasets.get(\"validation\", None),\n","            tokenizer=tokenizer,\n","            data_collator=data_collator,\n","            compute_metrics=None,  # Sin m√©tricas que requieran generaci√≥n\n","        )\n","\n","        # Entrenar de nuevo\n","        print(\"Reiniciando entrenamiento sin generaci√≥n durante evaluaci√≥n...\")\n","        train_result = trainer_no_generate.train()\n","\n","        # Guardar\n","        trainer_no_generate.save_model()\n","        tokenizer.save_pretrained(config.output_dir + \"_no_generate\")\n","\n","        print(f\"\\n‚úÖ Entrenamiento completado sin generaci√≥n\")\n","        print(f\"üìÅ Modelo guardado en: {config.output_dir}_no_generate\")\n","\n","    else:\n","        print(f\"\\n‚ùå ERROR durante el entrenamiento: {e}\")\n","        import traceback\n","        traceback.print_exc()\n","\n","except Exception as e:\n","    print(f\"\\n‚ùå ERROR inesperado: {e}\")\n","    import traceback\n","    traceback.print_exc()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":944},"id":"_9tL8solKTC5","executionInfo":{"status":"ok","timestamp":1770134620534,"user_tz":-60,"elapsed":277433,"user":{"displayName":"Guillermo Barrio","userId":"06925154007558381895"}},"outputId":"b1ffebab-1097-42ef-a114-df9fe1e819d5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","======================================================================\n","üöÄ INICIANDO ENTRENAMIENTO SEGURO\n","======================================================================\n","Memoria GPU antes del entrenamiento: 0.89 GB / 23.80 GB\n","Pasos totales estimados: 540\n","Duraci√≥n estimada: 18.0 minutos (aproximadamente)\n","\n","======================================================================\n","‚è≥ ENTRENANDO... (esto puede tomar varios minutos)\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='540' max='540' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [540/540 04:29, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>100</td>\n","      <td>1.038000</td>\n","      <td>0.807598</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>0.690500</td>\n","      <td>0.674657</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>0.707100</td>\n","      <td>0.616378</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>0.565600</td>\n","      <td>0.593632</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>0.522900</td>\n","      <td>0.583102</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","üîç Evaluaci√≥n completada en paso 100\n","\n","üîç Evaluaci√≥n completada en paso 200\n","\n","üîç Evaluaci√≥n completada en paso 300\n","\n","üîç Evaluaci√≥n completada en paso 400\n","\n","üîç Evaluaci√≥n completada en paso 500\n"]},{"output_type":"stream","name":"stderr","text":["There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"]},{"output_type":"stream","name":"stdout","text":["***** train metrics *****\n","  epoch                    =        3.0\n","  total_flos               =  1225014GF\n","  train_loss               =     0.8151\n","  train_runtime            = 0:04:30.71\n","  train_samples_per_second =     15.958\n","  train_steps_per_second   =      1.995\n","\n","======================================================================\n","üéâ ¬°ENTRENAMIENTO COMPLETADO EXITOSAMENTE!\n","======================================================================\n","üìÅ Modelo guardado en: /content/drive/MyDrive/Practica_LLM_Engineering_25/t5_electoral_safe_20260203_1558\n","\n","üìä RESUMEN DEL ENTRENAMIENTO:\n","  √âpocas completadas: 3\n","  Pasos totales: 540\n","  P√©rdida final: 0.8151\n","  P√©rdida en validaci√≥n: N/A\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"vMmk4tkvKS7h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"JkQJYxhrKS1h"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Probamos el modelo T5 ya finetuneado con el dataset de Test. Obtenemos unas p√©rdidas bastante parecidas a las de train y validation, 0,57, con lo que no parece haber un overfitting claro.\n","\n","Vemos que obtenemos un resultado de Rouge de cero, lo cual pregunt√© al respecto a DeepSeek."],"metadata":{"id":"7Il6rsZeyS0l"}},{"cell_type":"code","source":["# ============================================================================\n","# CELDA 10: EVALUACI√ìN FINAL SEGURA\n","# ============================================================================\n","print(\"\\n\" + \"=\" * 70)\n","print(\"üìä EVALUACI√ìN FINAL SEGURA\")\n","print(\"=\" * 70)\n","\n","if \"test\" in tokenized_datasets and len(tokenized_datasets[\"test\"]) > 0:\n","    print(\"Evaluando en conjunto de test...\")\n","\n","    try:\n","        # Para evaluaci√≥n final, podemos usar generaci√≥n\n","        # pero con configuraci√≥n segura\n","\n","        # Configurar generaci√≥n segura para evaluaci√≥n final\n","        eval_args = Seq2SeqTrainingArguments(\n","            output_dir=config.output_dir + \"_final_eval\",\n","            per_device_eval_batch_size=config.batch_size,\n","            predict_with_generate=True,  # Usar generaci√≥n solo para evaluaci√≥n final\n","            generation_max_length=config.max_target_length,\n","            generation_num_beams=1,  # Usar beam=1 para mayor estabilidad\n","            dataloader_num_workers=0,\n","        )\n","\n","        eval_trainer = Seq2SeqTrainer(\n","            model=model,\n","            args=eval_args,\n","            eval_dataset=tokenized_datasets[\"test\"],\n","            tokenizer=tokenizer,\n","            data_collator=data_collator,\n","            compute_metrics=safe_compute_metrics,\n","        )\n","\n","        print(\"Realizando evaluaci√≥n final con generaci√≥n...\")\n","        eval_results = eval_trainer.evaluate()\n","\n","        print(\"\\nüìà RESULTADOS EN TEST:\")\n","        print(\"-\" * 40)\n","        for key, value in eval_results.items():\n","            if isinstance(value, float):\n","                print(f\"{key:20}: {value:.4f}\")\n","\n","        # Guardar resultados\n","        with open(f\"{config.output_dir}/test_results.json\", \"w\") as f:\n","            json.dump(eval_results, f, indent=2)\n","\n","        print(f\"‚úÖ Resultados guardados en: {config.output_dir}/test_results.json\")\n","\n","    except Exception as e:\n","        print(f\"‚ö†Ô∏è  Error en evaluaci√≥n final: {e}\")\n","        print(\"Realizando evaluaci√≥n simple (solo loss)...\")\n","\n","        # Evaluaci√≥n simple sin generaci√≥n\n","        try:\n","            simple_eval_args = Seq2SeqTrainingArguments(\n","                output_dir=config.output_dir + \"_simple_eval\",\n","                per_device_eval_batch_size=config.batch_size,\n","                predict_with_generate=False,\n","                dataloader_num_workers=0,\n","            )\n","\n","            simple_eval_trainer = Seq2SeqTrainer(\n","                model=model,\n","                args=simple_eval_args,\n","                eval_dataset=tokenized_datasets[\"test\"],\n","                tokenizer=tokenizer,\n","                data_collator=data_collator,\n","            )\n","\n","            simple_results = simple_eval_trainer.evaluate()\n","            print(f\"üìä P√©rdida en test: {simple_results['eval_loss']:.4f}\")\n","\n","        except Exception as e2:\n","            print(f\"‚ö†Ô∏è  Error incluso en evaluaci√≥n simple: {e2}\")\n","else:\n","    print(\"‚ö†Ô∏è  No hay conjunto de test para evaluaci√≥n\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":523},"id":"mky67Qu3OyuQ","executionInfo":{"status":"ok","timestamp":1770135025000,"user_tz":-60,"elapsed":158024,"user":{"displayName":"Guillermo Barrio","userId":"06925154007558381895"}},"outputId":"0cee7c32-23b8-49b5-fcb7-cfd8a60ca95e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","======================================================================\n","üìä EVALUACI√ìN FINAL SEGURA\n","======================================================================\n","Evaluando en conjunto de test...\n","Realizando evaluaci√≥n final con generaci√≥n...\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='45' max='45' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [45/45 02:14]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n","\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n","\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n","\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice:"]},{"name":"stdout","output_type":"stream","text":[" 3\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: You chose \"Don't visualize my results\"\n","\u001b[34m\u001b[1mwandb\u001b[0m: Using W&B in offline mode.\n","\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.24.0"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["W&B syncing is set to <code>`offline`<code> in this directory. Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing.<br>Run data is saved locally in <code>/content/wandb/offline-run-20260203_161022-0lks4pq9</code>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","üìà RESULTADOS EN TEST:\n","----------------------------------------\n","eval_loss           : 0.5740\n","eval_rouge1         : 0.0000\n","eval_rouge2         : 0.0000\n","eval_rougeL         : 0.0000\n","eval_rougeLsum      : 0.0000\n","eval_runtime        : 142.2180\n","eval_samples_per_second: 1.2660\n","eval_steps_per_second: 0.3160\n","‚úÖ Resultados guardados en: /content/drive/MyDrive/Practica_LLM_Engineering_25/t5_electoral_safe_20260203_1558/test_results.json\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"Y_5fiIgJQGP6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Q11uNkShQGHK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"feIcULd7OyrT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["DeepSeek a√±adi√≥ esta celda para probar el modelo con preguntas/respuestas que no hubiese visto el modelo antes. Honestamente, no pens√© que fuese interesante cuando no hemos siquiera configurar el RAG, con lo que no la ejecut√©."],"metadata":{"id":"FvcptQnvy012"}},{"cell_type":"code","source":["# ============================================================================\n","# CELDA 11: PRUEBA DEL MODELO ENTRENADO\n","# ============================================================================\n","print(\"\\n\" + \"=\" * 70)\n","print(\"üß™ PRUEBA DEL MODELO ENTRENADO\")\n","print(\"=\" * 70)\n","\n","def test_trained_model(model_path, test_questions, max_examples=3):\n","    \"\"\"Prueba el modelo con preguntas de ejemplo\"\"\"\n","\n","    from transformers import T5Tokenizer, T5ForConditionalGeneration\n","    import torch\n","\n","    print(f\"Cargando modelo desde {model_path}...\")\n","\n","    try:\n","        tokenizer_test = T5Tokenizer.from_pretrained(model_path)\n","        model_test = T5ForConditionalGeneration.from_pretrained(model_path)\n","\n","        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        model_test = model_test.to(device)\n","        model_test.eval()\n","\n","        print(f\"‚úÖ Modelo cargado en {device}\")\n","\n","    except Exception as e:\n","        print(f\"‚ùå Error cargando modelo: {e}\")\n","        return\n","\n","    print(\"\\nüìù PROBANDO CON PREGUNTAS DE EJEMPLO:\")\n","\n","    for i, question in enumerate(test_questions[:max_examples]):\n","        print(f\"\\n--- Ejemplo {i+1} ---\")\n","        print(f\"Pregunta: {question}\")\n","\n","        # Asegurar formato correcto\n","        if not question.strip().endswith(\"respuesta:\"):\n","            question = question.strip() + \" respuesta:\"\n","\n","        try:\n","            # Tokenizar\n","            inputs = tokenizer_test(\n","                question,\n","                return_tensors=\"pt\",\n","                truncation=True,\n","                max_length=config.max_input_length,\n","                padding=True\n","            ).to(device)\n","\n","            # Generar\n","            with torch.no_grad():\n","                outputs = model_test.generate(\n","                    **inputs,\n","                    max_length=config.max_target_length,\n","                    num_beams=config.num_beams,\n","                    temperature=config.temperature,\n","                    do_sample=True,\n","                    repetition_penalty=config.repetition_penalty,\n","                    early_stopping=True\n","                )\n","\n","            # Decodificar\n","            response = tokenizer_test.decode(outputs[0], skip_special_tokens=True)\n","\n","            # Extraer solo la respuesta (lo que viene despu√©s del prompt)\n","            prompt_length = len(tokenizer_test.decode(inputs.input_ids[0], skip_special_tokens=True))\n","            response_only = response[prompt_length:].strip()\n","\n","            print(f\"ü§ñ Respuesta: {response_only}\")\n","\n","        except Exception as e:\n","            print(f\"‚ùå Error generando respuesta: {e}\")\n","\n","# Preguntas de prueba\n","test_questions = [\n","    \"pregunta: ¬øQu√© porcentaje de votos obtuvo el PSOE en el municipio de Madrid en 2019?\",\n","    \"pregunta: ¬øC√≥mo evolucion√≥ la participaci√≥n electoral entre 2016 y 2019?\",\n","    \"pregunta: ¬øCu√°l fue el partido m√°s votado en las √∫ltimas elecciones?\",\n","]\n","\n","# Probar el modelo\n","test_trained_model(config.output_dir, test_questions)\n"],"metadata":{"id":"nR6fVXVVOyoo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"S5Lao3w1Oylo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"N5zNWxwqOyi6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Despu√©s de preguntarle a DeepSeek por el cero obtenido con Rouge, me recomend√≥ esta celda adicional, donde examina ejemplos de test, y que obtiene coeficientes de Rouge superiores a cero. El problema es que en esos casos recoge en la respuesta datos que le hemos pasado en la pregunta, con lo que, sinceramente, no me parece √∫til esta m√©trica."],"metadata":{"id":"_8CG7_lmzQCW"}},{"cell_type":"code","source":["# A√ëADE ESTA CELDA PARA DIAGNOSTICAR ROUGE\n","print(\"\\n\" + \"=\" * 70)\n","print(\"üîç DIAGN√ìSTICO DE M√âTRICAS ROUGE\")\n","print(\"=\" * 70)\n","\n","def debug_rouge_metrics(model_path, test_dataset, num_examples=5):\n","    \"\"\"Debug detallado de las m√©tricas ROUGE\"\"\"\n","\n","    from transformers import T5Tokenizer, T5ForConditionalGeneration\n","    import torch\n","\n","    # Cargar modelo\n","    tokenizer = T5Tokenizer.from_pretrained(model_path)\n","    model = T5ForConditionalGeneration.from_pretrained(model_path)\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model = model.to(device)\n","    model.eval()\n","\n","    print(f\"Analizando {num_examples} ejemplos del test set...\\n\")\n","\n","    for i in range(min(num_examples, len(test_dataset))):\n","        example = test_dataset[i]\n","\n","        print(f\"\\n--- Ejemplo {i+1} ---\")\n","        print(f\"üìù INPUT ORIGINAL:\")\n","        print(f\"   {example['input_text'][:150]}...\")\n","        print(f\"\\nüéØ TARGET ESPERADO:\")\n","        print(f\"   {example['target_text']}\")\n","\n","        # Generar respuesta del modelo\n","        inputs = tokenizer(\n","            example['input_text'],\n","            return_tensors=\"pt\",\n","            truncation=True,\n","            max_length=256\n","        ).to(device)\n","\n","        with torch.no_grad():\n","            outputs = model.generate(\n","                **inputs,\n","                max_length=128,\n","                num_beams=1,\n","                temperature=0.7,\n","                do_sample=True\n","            )\n","\n","        generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","\n","        # Extraer solo la parte de respuesta\n","        input_decoded = tokenizer.decode(inputs.input_ids[0], skip_special_tokens=True)\n","        if generated.startswith(input_decoded):\n","            generated_response = generated[len(input_decoded):].strip()\n","        else:\n","            generated_response = generated\n","\n","        print(f\"\\nü§ñ RESPUESTA GENERADA:\")\n","        print(f\"   {generated_response}\")\n","\n","        # Calcular ROUGE manualmente para ver qu√© pasa\n","        from rouge_score import rouge_scorer\n","\n","        scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n","        scores = scorer.score(example['target_text'], generated_response)\n","\n","        print(f\"\\nüìä ROUGE manual:\")\n","        print(f\"   Rouge-1: {scores['rouge1'].fmeasure:.4f}\")\n","        print(f\"   Rouge-2: {scores['rouge2'].fmeasure:.4f}\")\n","        print(f\"   Rouge-L: {scores['rougeL'].fmeasure:.4f}\")\n","\n","        # Verificar si hay n√∫meros en ambas respuestas\n","        import re\n","        target_numbers = set(re.findall(r'\\d+\\.?\\d*', example['target_text']))\n","        generated_numbers = set(re.findall(r'\\d+\\.?\\d*', generated_response))\n","\n","        print(f\"\\nüî¢ N√∫meros en target: {target_numbers}\")\n","        print(f\"üî¢ N√∫meros en generado: {generated_numbers}\")\n","        print(f\"üìê Coincidencia num√©rica: {len(target_numbers.intersection(generated_numbers))}/{len(target_numbers)}\")\n","\n","        print(\"-\" * 80)\n","\n","# Ejecutar diagn√≥stico\n","debug_rouge_metrics(config.output_dir, dataset[\"test\"], num_examples=6)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TiHVmcpVOyfy","executionInfo":{"status":"ok","timestamp":1770136020283,"user_tz":-60,"elapsed":14648,"user":{"displayName":"Guillermo Barrio","userId":"06925154007558381895"}},"outputId":"d464e113-0884-4f0e-959d-7bdd612b4e78"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"output_type":"stream","name":"stdout","text":["\n","======================================================================\n","üîç DIAGN√ìSTICO DE M√âTRICAS ROUGE\n","======================================================================\n","Analizando 6 ejemplos del test set...\n","\n","\n","--- Ejemplo 1 ---\n","üìù INPUT ORIGINAL:\n","   ### Instrucci√≥n:\n","¬øC√≥mo evolucion√≥ el porcentaje de voto a VOX en Medina del Campo entre Noviembre 2019 y Abril 2019?\n","\n","### Contexto:\n","Municipio: Medina ...\n","\n","üéØ TARGET ESPERADO:\n","   El VOX en Medina del Campo pas√≥ de 19.86% en Noviembre 2019 a 13.75% en Abril 2019. Esto representa una disminuci√≥n de 6.11 puntos porcentuales (30.8% menos).\n","\n","ü§ñ RESPUESTA GENERADA:\n","   <pad> El VOX en Medina del Campo pas√≥ de 37.53% en Noviembre 2019 a 33.79% en Abril 2019. Esto representa un incremento de 6.28% (de incremento porcentual). Esto representa un incremento de 1.64% (de incremento porcentual). Esto representa un incremento de 4.09 puntos porcentuales (0.25% disminuci√≥n). Esto representa un incremento de 1.39 puntos porcentuales (0.66% disminuci√≥n porcent\n","\n","üìä ROUGE manual:\n","   Rouge-1: 0.4646\n","   Rouge-2: 0.3505\n","   Rouge-L: 0.4444\n","\n","üî¢ N√∫meros en target: {'19.86', '2019.', '13.75', '2019', '30.8', '6.11'}\n","üî¢ N√∫meros en generado: {'0.25', '2019.', '1.39', '4.09', '1.64', '37.53', '2019', '0.66', '33.79', '6.28'}\n","üìê Coincidencia num√©rica: 2/6\n","--------------------------------------------------------------------------------\n","\n","--- Ejemplo 2 ---\n","üìù INPUT ORIGINAL:\n","   ### Instrucci√≥n:\n","¬øCu√°l fue el cambio porcentual del voto a VOX en Mahora entre las elecciones de Noviembre 2019 y Abril 2019?\n","\n","### Contexto:\n","Municipio...\n","\n","üéØ TARGET ESPERADO:\n","   El VOX en Mahora pas√≥ de 20.92% en porcentaje de voto en la elecci√≥n Noviembre 2019 al 14.27% en Abril 2019. Esto representa un cambio del -31.8% ( de disminuci√≥n porcentual).\n","\n","ü§ñ RESPUESTA GENERADA:\n","   <pad> El VOX en Mahora pas√≥ de 0.00% en porcentaje de voto en la elecci√≥n Noviembre 2019 al 0.00% en Abril 2019. Esto representa un cambio del -2.8% ( de disminuci√≥n porcentual). Esto representa un cambio del -98.1% ( de disminuci√≥n porcentual). Esto representa un cambio del +/-0.04% (de disminuci√≥n porcentual) del - -6% (de disminu\n","\n","üìä ROUGE manual:\n","   Rouge-1: 0.6186\n","   Rouge-2: 0.5474\n","   Rouge-L: 0.6186\n","\n","üî¢ N√∫meros en target: {'2019.', '31.8', '20.92', '2019', '14.27'}\n","üî¢ N√∫meros en generado: {'2019.', '2.8', '2019', '0.00', '0.04', '6', '98.1'}\n","üìê Coincidencia num√©rica: 2/5\n","--------------------------------------------------------------------------------\n","\n","--- Ejemplo 3 ---\n","üìù INPUT ORIGINAL:\n","   ### Instrucci√≥n:\n","¬øCu√°l es el resultado de la columna IU en la secci√≥n 0704004061 en la elecci√≥n Abril 2019?\n","\n","### Contexto:\n","Datos de 0704004061 en Abri...\n","\n","üéØ TARGET ESPERADO:\n","   El resultado de la columna IU de la secci√≥n 0704004061 en la elecci√≥n Abril 2019 es 0\n","\n","ü§ñ RESPUESTA GENERADA:\n","   <pad> El resultado de la columna IU de la secci√≥n 0704004061 en la elecci√≥n Abril 2019 es 06.0.01470100217703723762771.036779870.0806898909753790265963631.0809667778907698077678697676786978807808078767971787973787870797970807767678797979797979808786779797979\n","\n","üìä ROUGE manual:\n","   Rouge-1: 0.8636\n","   Rouge-2: 0.8095\n","   Rouge-L: 0.8636\n","\n","üî¢ N√∫meros en target: {'0', '2019', '0704004061'}\n","üî¢ N√∫meros en generado: {'01470100217703723762771.036779870', '0806898909753790265963631.0809667778907698077678697676786978807808078767971787973787870797970807767678797979797979808786779797979', '06.0', '2019', '0704004061'}\n","üìê Coincidencia num√©rica: 2/3\n","--------------------------------------------------------------------------------\n","\n","--- Ejemplo 4 ---\n","üìù INPUT ORIGINAL:\n","   ### Instrucci√≥n:\n","¬øEn qu√© municipio de Almer√≠a fue mayor % PP en Abril 2019: Alm√≥cita o Uleila del Campo?\n","\n","### Contexto:\n","A√±o: Abril 2019, Variable: % P...\n","\n","üéØ TARGET ESPERADO:\n","   En Abril 2019, % PP fue mayor en Uleila del Campo. Valores: Alm√≥cita: 10.19%, Uleila del Campo: 23.11%. Diferencia: 12.92 puntos.\n","\n","ü§ñ RESPUESTA GENERADA:\n","   <pad> En Abril 2019, % PP fue mayor en Almera. Valores: Alm√≥cita: 24.04%, Uleila del Campo: 28.08%. Diferencia: 0.1 puntos. Valores: Almera: 33.32%. Diferencia: 0.28 puntos. Diferencia: 0.189 puntos. Diferencia: 1.22 puntos. Diferencia: 2.38 puntos. Diferencia: 1.18 puntos. Valores: 1.\n","\n","üìä ROUGE manual:\n","   Rouge-1: 0.4110\n","   Rouge-2: 0.2817\n","   Rouge-L: 0.4110\n","\n","üî¢ N√∫meros en target: {'12.92', '2019', '10.19', '23.11'}\n","üî¢ N√∫meros en generado: {'1.18', '0.28', '2019', '0.1', '1.22', '24.04', '1.', '2.38', '0.189', '33.32', '28.08'}\n","üìê Coincidencia num√©rica: 1/4\n","--------------------------------------------------------------------------------\n","\n","--- Ejemplo 5 ---\n","üìù INPUT ORIGINAL:\n","   ### Instrucci√≥n:\n","Considerando los datos de los municipios de la provincia de Sevilla, ¬øc√≥mo se relaciona la renta por persona (2017) con el apoyo al P...\n","\n","üéØ TARGET ESPERADO:\n","   El an√°lisis de los datos muestra una relaci√≥n negativa moderada entre la renta por persona y el apoyo al PSOE, y una relaci√≥n positiva m√°s clara con la participaci√≥n electoral. 1. **Cuantificaci√≥n de las relaciones:** *   **Renta y voto al PSOE:** Se observa una tendencia inversa. En municipios con renta por persona baja (√âcija: 7.907‚Ç¨, Coria del R√≠o: 6.366‚Ç¨, Pedrera: 7.322‚Ç¨), el PSOE obtiene sus porcentajes m√°s altos (50%, 50% y 40%, respectivamente). Por el contrario, en las secciones de Sevilla con mayor renta (17.745‚Ç¨ y 11.108‚Ç¨), el apoyo al PSOE cae al 10% y 40%. La correlaci√≥n es evidente, aunque no perfecta (ej. Gines, con 13.994‚Ç¨, tiene un 20% para el PSOE, similar a secciones de Sevilla con menor renta). El apoyo medio al PSOE (Œº=40%) se concentra en municipios con renta cercana o por debajo de la media provincial (Œº=9.778,6‚Ç¨). *   **Renta y participaci√≥n:** La relaci√≥n es positiva. Los tres municipios/secciones con mayor participaci√≥n (90% en Gines y en una secci√≥n de Sevilla de alta renta, 80% en Los Palacios y en otra secci√≥n de Sevilla de renta media-alta) tienen rentas por persona de 13.994‚Ç¨, 17.745‚Ç¨, 7.921‚Ç¨ y 11.108‚Ç¨ respectivamente. Los de menor participaci√≥n (60% en Coria del R√≠o, 70% en varios) tienen las rentas m√°s bajas. La participaci√≥n media (Œº=70%) parece un umbral asociado a la renta media. 2. **Mecanismos causales posibles:** *   **Base de clase y mensaje pol√≠tico:** El PSOE, como partido tradicional de centro-izquierda, puede mantener una base electoral m√°s fuerte en zonas de menor renta, donde su discurso sobre protecci√≥n social, servicios p√∫blicos y redistribuci√≥n resuena m√°s. Las zonas de mayor renta podr√≠an mostrar mayor preferencia por opciones conservadoras (PP, Cs) o, en un caso (Sevilla, renta 17.745‚Ç¨), una fragmentaci√≥n del voto entre PP, Cs y VOX. *   **Movilizaci√≥n y recursos:** La mayor participaci√≥n en zonas de mayor renta puede explicarse por la \"teor√≠a de los recursos\" (mayor educaci√≥n, estabilidad, capital social que facilita la participaci√≥n pol√≠tica). La menor participaci√≥n en zonas deprimidas puede reflejar desafecci√≥n, dificultades log√≠sticas o una percepci√≥n de menor eficacia pol√≠tica. 3. **Heterogeneidad geogr√°fica (caso de Sevilla):** *   La ciudad de Sevilla presenta una **heterogeneidad interna extrema**, actuando como un microcosmos de las tendencias provinciales. En sus secciones: *   **Alta renta (17.745‚Ç¨):** Bajo PSOE (10%), alto PP (40%), alta participaci√≥n (90%). *   **Renta media-alta (11.108‚Ç¨):** PSOE moderado (40%), PP moderado (20%), participaci√≥n alta (80%). *   **Renta media-baja (10.974‚Ç¨):** PSOE bajo (30%), voto muy fragmentado (PSOE, VOX, Cs, UP cada uno entre 20-30%), participaci√≥n media (70%). *   **Baja renta (6.561‚Ç¨):** PSOE muy alto (60%), participaci√≥n media (70%). *   Esto sugiere que el **gradiente socioecon√≥mico intraurbano es un predictor m√°s potente del comportamiento electoral que la simple divisi√≥n urbano/rural**. La fragmentaci√≥n del voto en secciones de renta media podr√≠a indicar mayor competitividad y volatilidad pol√≠tica. 4. **Implicaciones para pol√≠ticas p√∫blicas:** *   **Focalizaci√≥n de mensajes y campa√±as:** Los partidos, especialmente el PSOE, deben adaptar su comunicaci√≥n: en zonas de baja renta, enfatizar la defensa de servicios p√∫blicos y protecci√≥n social; en zonas de renta media y alta, abordar temas como gesti√≥n econ√≥mica, fiscalidad y modernizaci√≥n. *   **Pol√≠ticas para combatir la abstenci√≥n:** Las iniciativas para aumentar la participaci√≥n (informaci√≥n, accesibilidad a votar) deber√≠an priorizar √°reas de baja renta, donde la desmovilizaci√≥n es mayor. Programas de educaci√≥n c√≠vica y reforzar la conexi√≥n entre pol√≠ticas locales y mejora tangible de la vida cotidiana son clave. *   **Cohesi√≥n territorial y social:** La marcada divisi√≥n electoral seg√∫n la renta, visible incluso dentro de una misma ciudad, subraya el riesgo de fractura social. Pol√≠ticas de cohesi√≥n que reduzcan las desigualdades socioecon√≥micas (acceso a educaci√≥n, empleo de calidad, vivienda) podr√≠an, a largo plazo, moderar esta polarizaci√≥n geogr√°fica del voto, creando un electorado menos segmentado por nivel de ingresos. En conclusi√≥n, los datos confirman que la renta es un determinante significativo, aunque no exclusivo, del voto y la participaci√≥n en la provincia de Sevilla. La heterogeneidad dentro de la capital amplifica este patr√≥n, se√±alando la necesidad de un an√°lisis electoral de alta resoluci√≥n espacial y de pol√≠ticas que atiendan a las realidades socioecon√≥micas espec√≠ficas de cada territorio.\n","\n","ü§ñ RESPUESTA GENERADA:\n","   <pad> El an√°lisis de los datos revela una correlaci√≥n compleja y no lineal entre la renta por persona y el apoyo al PSOE y la participaci√≥n electoral. El municipio de Sevilla posee una relaci√≥n negativa entre la renta por persona y el voto al PSOE. El municipio con mayor renta por persona era el municipio de Sevilla. El apoyo al PSOE tiene\n","\n","üìä ROUGE manual:\n","   Rouge-1: 0.1429\n","   Rouge-2: 0.0907\n","   Rouge-L: 0.1048\n","\n","üî¢ N√∫meros en target: {'40', '2.', '70', '13.994', '11.108', '90', '7.322', '20', '7.921', '50', '6.561', '1.', '80', '6', '10.974', '9.778', '60', '7.907', '6.366', '30', '17.745', '3.', '4.', '10'}\n","üî¢ N√∫meros en generado: set()\n","üìê Coincidencia num√©rica: 0/24\n","--------------------------------------------------------------------------------\n","\n","--- Ejemplo 6 ---\n","üìù INPUT ORIGINAL:\n","   ### Instrucci√≥n:\n","¬øQu√© porcentaje de votos obtuvo el PP en la secci√≥n 0303104005 en las elecciones de Abril 2019?\n","\n","### Contexto:\n","Datos de 0303104005 en...\n","\n","üéØ TARGET ESPERADO:\n","   El partido PP obtuvo en la secci√≥n 0303104005 el 25.43% de votos en las eleccione de Abril 2019.\n","\n","ü§ñ RESPUESTA GENERADA:\n","   <pad> El partido PP obtuvo en la secci√≥n 0303104005 el 0.00% de votos en las eleccione de Abril 2019. Esto representa un porcentaje de votos de 905,217.0. Esto representa una disminuci√≥n de 759.75%, y una disminuci√≥n total de 589.25%. Esto representa una disminuci√≥n de 835.95%. Estos elecciones de 437\n","\n","üìä ROUGE manual:\n","   Rouge-1: 0.4810\n","   Rouge-2: 0.4156\n","   Rouge-L: 0.4557\n","\n","üî¢ N√∫meros en target: {'2019.', '0303104005', '25.43'}\n","üî¢ N√∫meros en generado: {'905', '2019.', '835.95', '0.00', '217.0', '437', '759.75', '0303104005', '589.25'}\n","üìê Coincidencia num√©rica: 2/3\n","--------------------------------------------------------------------------------\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"xmnjXVw0Oycw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"BVCPA4hEOyZ0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"SFjsXuDJOyWu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"80IZHGkROyTT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"3LelYzmgOyQF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"wx-_vBTxOyM4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"RZFF7N6WOyJb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"umwcY8adOyFQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"kSJosBCZOyBi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"-XKQdgNAOx-A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"7JolQDB0Ox6U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"FEvRXl8yOx2Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"EoYwExZXOxyA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"LHOkO_EROxpI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"6uTkA5p1KStL"},"execution_count":null,"outputs":[]}]}